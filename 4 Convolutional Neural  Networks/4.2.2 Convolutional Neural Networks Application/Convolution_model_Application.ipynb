{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Create a mood classifer using the TF Keras Sequential API\n",
    "- Build a ConvNet to identify sign language digits using the TF Keras Functional API\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a __binary__ classification problem\n",
    "- Build and train a ConvNet in TensorFlow for a __multiclass__ classification problem\n",
    "- Explain different use cases for the Sequential and Functional APIs\n",
    "\n",
    "To complete this assignment, you should already be familiar with TensorFlow. If you are not, please refer back to the **TensorFlow Tutorial** of the third week of Course 2 (\"**Improving deep neural networks**\").\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/convolutional-neural-networks/supplement/DS4yP/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "    - [1.1 - Load the Data and Split the Data into Train/Test Sets](#1-1)\n",
    "- [2 - Layers in TF Keras](#2)\n",
    "- [3 - The Sequential API](#3)\n",
    "    - [3.1 - Create the Sequential Model](#3-1)\n",
    "        - [Exercise 1 - happyModel](#ex-1)\n",
    "    - [3.2 - Train and Evaluate the Model](#3-2)\n",
    "- [4 - The Functional API](#4)\n",
    "    - [4.1 - Load the SIGNS Dataset](#4-1)\n",
    "    - [4.2 - Split the Data into Train/Test Sets](#4-2)\n",
    "    - [4.3 - Forward Propagation](#4-3)\n",
    "        - [Exercise 2 - convolutional_model](#ex-2)\n",
    "    - [4.4 - Train the Model](#4-4)\n",
    "- [5 - History Object](#5)\n",
    "- [6 - Bibliography](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "As usual, begin by loading in the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "from test_utils import summary, comparator\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Load the Data and Split the Data into Train/Test Sets\n",
    "\n",
    "You'll be using the Happy House dataset for this part of the assignment, which contains images of peoples' faces. Your task will be to build a ConvNet that determines whether the people in the images are smiling or not -- because they only get to enter the house if they're smiling!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the images contained in the dataset. Images are **64x64** pixels in RGB format (3 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO29a6xk2XUettY5p573ffs9090zfIz5ECWOpDFNgYIxJk2DcQzzlwILcMAEBOaPbMiIA5O0gQAOYIBGAscJECQYxIoJWLFMWJZJC45tZmI6siSS07RIaobDeWjYnH7f9731PHUeOz+quta31r1VfWe6u24Pa33Axd2n9q599tnn7Dpr7bXWtziEQA6H46cf0UkPwOFwzAa+2B2OOYEvdodjTuCL3eGYE/hidzjmBL7YHY45wX0tdmb+DDO/ysxvMPMXH9SgHA7Hgwe/Uzs7M8dE9BoRfZqIrhPRi0T0qyGEHz644TkcjgeF5D6++zEieiOE8CYRETP/FhF9logmLvb19ZVw6eJZIiIKpalkKZamLpTyg1SpVrFm4uCiyFwaH92Wme0n0nvITdvkyDKRvRg8Vzyxf6JC1ZRBjtW5DglgdsyTzj2t3YPFtJcGTrFtxyzXFsz95IlH9lw4/8cVVu3cTHvpzW4e7xdXr16lra2tIwd8P4v9cSK6BsfXiejPTPvCpYtn6f/+V/8LERFlqZlcuEdpqhdP2hmMyxeeeGJcLvOBasdwOc2ldV0X4cKScydJRY+D5TgbbKuqSm1NypXT0lvoq3b4IxHFC7p/qkC7jqrpZXvjci2Rc8VRQw+RKupInZsGx2qn8U61OZnHLMsn1kXQfV7oH7hqRX68C8pUXax+8GT8ZdDtAsn8R2znyv7YSs2k8R4Gzo/93qQfmgfx42HurXoZHP0CeOaZZyb2dj86+1EjPnSFzPwcM19h5ivbOwf3cTqHw3E/uJ83+3UiugTHF4nopm0UQnieiJ4nIvq5n30qZMXwDVAU+nem3+uNy6trZ1VdJUrH5TSVt+3y2nnVrhjI26XMW6qOY/mFz/pdGV9jUbdjeWt02l1Vt1qTcQ1y+eFio5PEMYim5q3PoE7YX+dm9TQcoboyTRWwwu6kW3r/b+90YCWYAGX9rbwQCaNeq8soDg0DJIBDbyt5kxVqHnW7iFF60mMs4c3Lak7tuwolDj2Hh9UoXfv2Pic6/E483pv+8NtcUJbFxLq7uJ83+4tE9BQzv4eZq0T0V4jo6/fRn8PheIh4x2/2EELOzH+NiP4tDV89vxFCePmBjczhcDxQ3I8YTyGEf01E//oBjcXhcDxE3Ndif7uIk5iW14a7zN0DvRN9a3N3XF5cv6TqOBFdPM1Fp964eUOfYCC6/eLykqqqLcjufKct+nxe6F3kak30v6WlFVXX78hueaUmu75RpHX2AvTEmGp6jBHolEFPvzKaMezos76W4+6sB7W7rcfIqg/7GIQjy5ExU3IUH9mOiKgE+ynuU0RGaS/AIhEZrRL17RKsDDFbCwe0C21Vo8152IfetUcryeF9j2lmPzpm3eR9lne2Oz/J3DvFHH3Mszgcjnc5fLE7HHOCmYrxHEVUbw5FsDLXvzMhEzFqb+O2qsv7IuLXVkWkzXa1KnD2sQvSnxEX40TMP4urItIniW5XrYt5bX9LWxLTvoj/6+eelIrIik5i4imtww2YSJJImxgjkjESi4ich5ZpJ6rB0BACXyPlrgYVWl3R3npG1VAiopSTyjSTlJ5HjuB8LH0E4zWYlzg/2kkqsLSNYIyx8YYsgqhXGZSJ9HUmSnTX54p5Gc9MGpPVFSLjlDWxHeL+PSAPOSyOTcYuxjsccw9f7A7HnMAXu8MxJ5ipzk7ERGGo/5SUqppaInpd0d1UdfFKE3oQHalR1+aT5sqZcfnO9bdUXSUWfbgkcc3NU+OKGkQHbi5rV1qMmanVoY6NWQv0y2B0Q6yLWPevhgFms0MmL9DleIoZhyGoJy90XAIGkxRBuwXXYH+DQc897DaKZj4TwUdoLhVzWJbrPQx0U22neg+mVpV7U0vkGYhgfEREaSku1MHo8zmcr4zkXjQTa/bE+2TdkydH1bG67nf67rRmtEnAe2uDZGwg0mH4m93hmBP4Ync45gSzFeNDSWU2FN+rkRaVqiD2NdZ1LHqyJKJke0PMcBFpMX5vS0x2fTDXEREN+hB5FctlZ5lWJ4p8a1xeMtF3CcReBxjvXdVk3H+EpjFrmpks+qLnFkZoTY7J1v0dPhaRtii0iW6QiRhYrVpzzdGPxWFyCVC9Si2C92EeIzAPDkotbmYwBZVkVddlIoLn0H9szF3tDFUUrSY0QPxPInkGQtDqFYFaaU2RmhfAmIxhTniquW0a3gnhyGQSkEnwN7vDMSfwxe5wzAlmKsaHMlDeH+6EVxs6mOHskxfH5eqyDkBZWBBShxQ2sK+9/j3VLk1FdFw5vazqCMScAcSHrBiVAXc5rWhUb8rJy0LExRhExVEtlI/PQadvx2TvNI3jeWqVpRZ9r96UHeyz67ru1Ioco+Viv6U9+ZYXROyuVLToGwWZkwDjzwpNKZUGGWOzogN+cgh4aSQidrf6t1S7AQQzdQba6rDUEPF8JZJzxZG+5phxXFaUnqZGIdBT0Ko8eG+nkVdME+mPS531Tls4HI6fCvhidzjmBL7YHY45wYyj3mKqLg515N0DTTzRyfbH5WrXkDSCrrW88r5x+dIHfla1O9h/fVxuVLUO2dkH01AMHm7G42pv84700eypuoUVoXeOKvA7aVRvjtDrTOuG6PEWpvLNT/PMmrYncDSqJmItgjm9dmdf1TXq4KVYk32K3X3tabe9syHtmnq+b++LGXQJ9mDimvYazDIkrdSm1CrJHsEi6PO7La2XD3Iwsyb62Tnoy3XWwXRar+n9ASY8tuZSvGeZqcN7geZSe8+m6eKTiC3eDj/+3fs72XTnb3aHY07gi93hmBPM1vQWCsqyochVshaHqrGIQBWTAaXfF0KCKH5xXG4sfFC1y/rCI9/vXlN1qxcuQDsRzwepNidVGyIGVWpanEOxOwTkWJscOBFK7clHh4gioH8+miTBcqKhKHnYu+5ocRG57ImILp8X0bqXadGvVoU5SKT/Jx9/TLX78VtXx+Xrt++oujKWeb0JZCQh1p58Swvipdjq6z5WqnLdSxAUM8g1z1wFzGhpoT0iK4nMd5nJvAWbCCgRM980EdzOt/KkPDaO62lnRfIpmWmO0aW/2R2OOYEvdodjTuCL3eGYE8xUZy/yAe1vXSciorSndfZuW9wh+7kmr6g2RU9aP3tuXF66oM04i2ui65cVQ9IYHW2iihOt7DQWRZeNjGKHKd0wi6sNOMoGMv440TpqjDo72+mHEyAZgdkT0DqlNQWhKQvdZbVuWUmQmFKPsd2Se9GLxcxVq+lxPHZe7kWloU1etbq4y751U+bj6jVNCNldEpKR9XVtisyABDJicc09taijEbNcTIeNWI+xXm1CWe5nfIg4BF1dp2Ea0cQ0bvjjmt6mYbJZ7nDq8cO455udmX+DmTeY+SX4bJ2Zv8HMr4/+r03rw+FwnDyOI8b/YyL6jPnsi0T0QgjhKSJ6YXTscDgeYdxTjA8h/H/M/KT5+LNE9Oyo/BUi+iYRfeFefTFHFEdDsWpxQfOILa+JuHX96o9VXYp53esitnaXtlW7xoKY3hYX3qfqykzMbQWYw9KBFveTipiXKsbLatATUbXIIG1Rol3okA+syE065yoKQVa1QNEdSS60OYmAMy4daPHtxo3rUob0WHc2tGq0vy/mq3SgVYEAaamqkNK62dQm0csXxZz51PtPq7rVhVPj8sJ7pHx+TYvxaVZCWddxJGPe60ikGwcdMVkCv3yaaq9HNJHWSOY+j7XakZE8c/X4ImmgOmfuhSJQmRz1Rip1tF1203jpj+7DEp+Ix+WD540/F0K4RUQ0+n/2Hu0dDscJ46HvxjPzc8x8hZmvbG8f3PsLDofjoeCd7sbfYeYLIYRbzHyBiDYmNQwhPE9EzxMRPf3RD4R6bbjDGjf0nt72HRDdCy2KLUJQS60pu+WdfeO1BRLtQlP3H9WAEANSE6U9LTrWG0JHbUkpCuCr67YlNVTTZHutVGQguSFrQJHc0kCzEgmlnQ2+yHIR1X7v9/4fVfftF38wLrc6Iu53+0b8LMNRRSLSlosqBK7EZqf7R6+8Mi5///tajP+5n33/uHzurFzXgLTqVamLOler64HsQkBUDwgrrBgcg0heMbyEmLKqn4nK1mycUu0CWCvKSAf8RIwqp92NxwyyqBLqa0FrwrTAJn2vJ9NWT6b1fvCBMF8nos+Nyp8joq+9w34cDseMcBzT2z8loj8kog8w83Vm/jwRfZmIPs3MrxPRp0fHDofjEcZxduN/dULVpx7wWBwOx0PETD3oojii+vIoZXOmPbqqELkUWXPVMqRbPiUb/zZdcQZKe79rCBmaQGzYFNNKvaG5ytGsVZZanycWc1upIq80aSWa3iqJ9tRi5SVnzETKCwo8Co2+/Qd/8PtS/tYVVbe3L3ojfi/LralGUJRGDwXPO25Lf3GkH5c6eMkVhn9/Z0fmvwbRa5cu672U935E7m2XdTRbqyNzkIAOXIm1PvzYaTGXRoZYc7e1My5XgZSjXtfPB3oUxqUxx7KMI2JtjiVG/nkUlG2kIkbO2Ui5o1Nk2/2BoDjxrVBeG7eaBPeNdzjmBL7YHY45wYwDYVLa236TiIjYSDLtbRHh4roe1gJkU63WQewO+rcqykWU3N95WdXFFRG1kwqK1tq8VhbSZ15or7OkIia2xVUJAkmq2hswlChuWZYE9P6y4pyI2t2OiJm//x9/T7X6w28LgcfevhZ90RsuTUXELI19LZQYMGNEPzjEKpswqddtQ9lwyi+LuF6CmnCwr8k8eqmoZZd+RnP9n1uRe1YOROVZWtDPxxoGL5H2rts/EOKMIpXnIx3sqHbVRMYRG/WQFU+hmQV4BlmZ6HQfOi2VJjBBAgw+9D2EPLeHufDumkWdg87hmHv4Ync45gS+2B2OOcGMCSdLGuRD09b2VW36iAvRbRdPaZfH5hK4sMZi+giFNiftt/5E2lX171gcoz4ouk+a6rxhBy05TjNtGju9JlF1MaT/jUrj9spghuJzug6m3KYNLkDXf/E73xqXv/Wtb6l2u7syd4UxqRVIqgh1pbG8obktWH1e6exgMjIECQH0w55xCyY4TipACNnXewxvvCJz9b4nf1nVffiDH4VxyFx1U/3sFGAqq5lIxbUVcfEtSzl3tWpTWEsfSaSvpZKIK3BMdg4wmq2majQwilGPPwTMrYeprnV8me5/dlFvDofjXQZf7A7HnGCmYnyel7S1NRSNt29qUeaJ918al9cunFF1CWZaAs+7KNYmjDgBcYi1OewuXz0RUaUi5rY40uIcRnxlufag22uJ6LS+LF5bZWlMgBGYtUpt4ilYRPV8oKP29vel7qUfSDrqjkmVHIGIXDFeVgnLcQzlNGixLwLxPDdiPJrpEhDdkylifBTMOEAFisC0lJn3y+amiPEv/eA1VffhD79XzgWeam/deFO1q9RAJcm0WtYbyJx+5Kk/PS4vNy+odmkmJrosM7z0MUQgmucKReugzHBkAFGGpQ31RhEfIu5YmyLD1LRf6bjVJPib3eGYE/hidzjmBDMV49N+Rld/OORIe2zZBI9A2qUy1iIhQwqfLBWejEpVk0ZgKqeopvvoQ90ekF5UqnoK1lcvj8uNhq7DwJsSgl02t/5EtUM1YbGh1ZU8l+91OlqMf/mPZYy3bgg5BnoGEhE9Dh5ki4n+ve71RdztgNNWN9LiXQ6WjNw48mWFtI3Ae6xqUkjFkAm2Emm5NYKmnMi96JmgmzupqCQ3b+r52NwS1WsRHCdvbGhRneFa6on2WEyAcKPflXaNmra04HuvUVudWHdoybCcL8vk2YyMepgXEkSVZ/qZqNeESCOOUYW13nQ4xyZIZvw8uhjvcMw9fLE7HHMCX+wOx5xgtuQVgaiWD3Wo2oI2m2Wl6KWbt7VO1u+JLvTYadGpmwvahFFh8bw76NxUdTu7V8flU+AJl5bai61SkaipfktHaC2CuQ1Vo85eR7VbXRXzzEGmTW99MAV19rVp77VXhfM9h3ZPrmhzz5NrMsYwMF6EQJzYAZNg3/yuI3dImVgPOlS4gcjiEDElED7YHFhQl1TAtGTScA1K0dn3dvV8/6cXfzgu/+LH5b4vJNrDkkFPv/zYe1Rd2hdTVgFEnb2u1tlL4EzlmiYciSJI52zOnWcw/0Hu2eFUznKdtaohNAG9X6eE1jp7UYBJ0EQq5qNrswGMCH+zOxxzAl/sDsecYLZifBTR0sJQDNpuay+l1QSydJ7S6XeSiog2vQMRlbodHbDQXJYgiJ0dLablhagNAxB9t3a1KpC1hQs9pFrkxOymCXCWLS3rgIV0IO129jVPegSsHW++ockxNm6JyH8KCDx+7pLmbauDKLy9aQKKwJMtgegX62mHhAylkc+DKsORcQtjDJKxPOlwnEA7m013rSZ1AxPUc+0tSV/1i78g3nTnVjXn++KCiMXLRgQvMBNvT+YqKzQ3PIGIT6e0SZcq4Mlmvpd1RPwvMjEdxjU9VwVkyo2a51Udg/l00BVPvkrTmjpFhcgHehzd7eFzVlg7KsDf7A7HnMAXu8MxJ/DF7nDMCWaqs8dRRIsLI1dSkzesAe6F/R3tNtkH4sRl4I2Pcq2zJ5HoQklP91+tihviQiR9tDpal2135VwDs6/Q6YvO1wadKWpo892pJdl/2DN9NJflOm9e06a3UIi+9cRZ0T0fP6N1yL0dcau1OlpZHE1KYQkqDEOFrsLvKRdNa9eBusia7zCqTsZUMSwadXS5NZFi3QOZu7An9/qDFy6rdlEi99oSmpQwP5WquDFzRZszaVH2dKKaduVG0tBsW6cTpxw45Qswifb1M5G2Za+J1/UzEZowfjCXFoned0ogfbblCtkeuRoXA0tEKThO+qdLzPzvmfkVZn6ZmX999Pk6M3+DmV8f/V+7V18Oh+PkcBwxPieivxlC+BARfZyIfo2ZP0xEXySiF0IITxHRC6Njh8PxiOI4ud5uEdGtUbnFzK8Q0eNE9FkienbU7CtE9E0i+sK0vpiZ6iORa3nZpNGpiHi7v6M9qYqeiDM7t8UzbnVVRyfd/rGYzQaGrGHlLJwvlssujAfamcvCN9Zb0pzyravi2YfSUi/VovQAvN96qfau2+qIKY6N+FyFULHzayKyVSL9mxwK5D2bLD6j2cx6VimJeZrblerfkFegJmB57JCsAcx+sUndVAV1rmJo1TIwYe5vilmyv6i54ZMGEEiY0ZdgUoug/6Sqn79oUcx5HOu6fE88G8NA3+sCPOgiUBnYmBhrNRlzbUGnt+aKjD+GcXFiVA1Qh3oHWgXMRg/koRwAgLe1QcfMTxLRzxPRt4no3OiH4O4PwtnJ33Q4HCeNYy92Zl4kot8mor8RQrC8OtO+9xwzX2HmK7sme4nD4ZgdjrXYeeip/9tE9JshhH8x+vgOM18Y1V8ggkgCQAjh+RDCMyGEZ9ZWFo9q4nA4ZoB76uw8JAv/R0T0SgjhH0DV14noc0T05dH/r92rryiKqNEc6sEra3rzfq8lpqxeR7sC1mqi01zfE32+bFiSQ2l3p631/oNIXFNPLYuJrm90e4aIpMcfe0qPEXTIsCftfvEDH1TtOhDpFplsyD9+S3LQxYbYsLMjun4dXIRzk946B7dSqytjJFoEun7EeiD4NcsHj3sJWgc0bDQ8sUodq/6NLtuoiV66ZNx2cXZSMKk1VzURo9Jlu31dB+eL6nLPokWtdUbgEhuMK2oJefci1kumAhF9UQxRarGekAo8w0lT7wkwpPVmyEdQZnoc6ZbsHbS3dDTllBRvct57N6FPENF/SUR/zMx3KU//Ng0X+VeZ+fNE9BYR/cox+nI4HCeE4+zG/0ea/LvxqQc7HIfD8bAwUw86jmOqLQ5FmJy1OLd5Q8xae3uaJ71TihjVSsTm9cRZLWavLT8xLh9c/QNVt7QilxpBaqjldR1BNYAUTFGixcX3v/9jcgB2nIWmITvgD0jZ/E6eOy/eX2+9+oaq27v6gnwPZOQs06rGAMX6Q0SPIMbD5/EhzncklSQN/CAAeYUV4xWrpN7+KUF8DmDqjBpahG00xLx5uqavM0tFPK83wQxqzsVVIH/om9RNEBGXnBLyEa5oEooApB9lT0cqKm2lps1hyJePo7IOiwwqBCfapMsRELlAf4MDvQ3W29uDZvoEY1PfYcL6I8fncDh+iuGL3eGYE8yWvIIjqjWGnkRXf/SqqtvfE9N9YkS9KAg/3coiiGyx3qW+vSXpg6pVHYjQrIq33ToG05zW3kyBRIS7fus7qq4KO/XNhlgT2ibYZVCK19zKghFbq+BJVdG/tUtN2LGF4A42HnSY5iquaJ50jiAwQ+3M2x136NNwuU9CYd4NPZC694yq0YGAlxg47lYizYH/vtOPj8vnGppzLd2XeTx/Tu4ZG274CLwvo0T3X1lEz0n4nvE0Kwfg6ZiYZVEHDjqT/ZVSOV8E99ru2mO2Xcw/YFH0RFQftDQxSZpDOq9C37O8cA46h8Mxgi92h2NO4Ivd4ZgTzFRnD6GkbBTBtrqmCQL64PlUv6C96y5dFPPY5raQEO7uGcJGyA3WCFpHrUHK36QietFPrn5Xt0OvPKMrnzoj+n0D9LjtfW0iKcD3q1b5GVXXbIhp6MwFvV+wBl6FS8vSf92Mo7Mp/d/pGN74juxjFBlExxm1PEI+eGMGxabIw7hrovu2+nJum4p5ETzGlkHPbRjiy7QvEY2PXX5S1VXWwGsO9nGihjZ1lhAVyTUdERcvQDRbInWlJf0oYX8j1ua1uIlRdVopjiEyjWtAjsFmX6EOx2a+UdEuwPTWY51boQdkpQNjfjwY5QMsXGd3OBy+2B2OOcFsPeiYqTYK8KgYAoINED+6N3T6p0ZdRN+Ljwt/+M6W9rQLS3I5seE6G0A6nqs/ERNdYTnRYhERG5EWxZogmg764M1UaJHw9Jrw3ldiPcYUgmSSmv6tffw9T47LEYkpaLelAyJeuSHqyvauJsdo9URdwfTQTZNueR3MXDXjdVWAKS4HETOzqhGYqJaNuWoJxNY6pMWuGLNZXMh4s442YS6tnRuXX7siqaAqH/tF1W7t8iXpz4wxqqMXJIjqiWkHQSw2MCiEyVzsymMNRGudxkmf21JslAFNavK9NNbcg/G68Cg2jVrWOD1cIxXj4YfwN7vDMSfwxe5wzAl8sTscc4KZ6uxlWVKvOzQR7G1os9kAbAalcd/kgdStLQnxRO9A67ILj4vutr91Q9X1e+LWOEhFT7/8p7QJsFbIlAx6mgihFosOubAgqYE5+YFql1REx9vYv6rHWJE9gVPLOmrvPR+Sa9t57Y/H5SjWrr/nFyRiqzYwvPfgZtsHMk0b2dYAPbqRGDdYmKsA5B4Vo8vmcJgY8yASSdYroLPXtM6eg9tnZ18TMixCau2QQo6/qzod9/r7PjwuR7HVlY9+nx0KDrMpp1VbSDlt/FFZfW8y0cdhKkxBDsQcGx1pd9uQv7W6QPCiHwmivDf63LB2AvzN7nDMCXyxOxxzgtmmf0oqtHB2KJrduK7Na1mJRAXaQ+qxD/zyuNxcElF9fVF7rvVBgrve1eYSzkV8rEA6n6Wgf+8akJ6pXNKefOungG8sEq+totAEGN22RPQ165rr7GBbRNXlup7+elVMLVXwzFq7oM0pb70uaX1LE21WgFjcByKHrmmHV73W1EQOOD8HXUjLbDjiUIHol8ZcBdM/yOVszVKL2cvISRdsZB48E2vibVgzkWdaeD7u++vtvOfQ23Aa2dsxiOCIqNPXkXm3tkSlRS7G9o7mho9gjZxq6HvWG6lbEU2OYPQ3u8MxJ/DF7nDMCWYqxmeDAd25PqTD3TOUvzmIQBefeI+qW1i/MC7vbQid7k+u/kS16xUS7J+aLK4HPfHOegyyot54TY8jK8U7bfUxzUHXyyW7bMKSzfNgR/9m5pmIX+vndTADxkDsbevxrxZCsNEsRQSv1rTIdv6CtKuZnfTdloiIPZDoeobsoAJECLnhM2vA7nk/FvF/qarF1AEI0DYwoxuAgw7KCdnsujI/tapWV/BsZV2CTMpD7R7Fd9a03Xd9LzDwprUvKlprV3PhEXh7hoF+rj70oQ8REVG9NnlJP4qz5HA4HgJ8sTsccwJf7A7HnGCmOntRlGNO+MJEP1XqorckhnjwzR9+e1zegJTNdzbuqHYV0Fc6PZPCB37WApiCEkNU0GqLzWj7hjaR9LrQ/4G4Nw262p3pzCWJ6LvxmiYNXDkjutb2gU5RhSmaVmB+8rbeV0Dzz0ZPmxg3wKOuC7/lSUXf6hqkl2oY4ssa6N/VRPpftl5mEDmGOjoRUQREmE24n6sLWtdcgEhCuzeBZJo5nLs45CU3LeX0o4eFup6DblfMuO0D2VtaXtDzsQCpqdtt7V7XaQ33icriPjzomLnOzN9h5u8z88vM/HdHn68z8zeY+fXR/7V79eVwOE4OxxHjUyL6ZAjho0T0NBF9hpk/TkRfJKIXQghPEdELo2OHw/GI4ji53gJJQs3K6C8Q0WeJ6NnR518hom8S0Rem9cVxTNXlodkoGWjxMwYx8/bGlqrb2pbjAXKHVUwqHiBQYEPWUML5bt4RwodTy9pbL0AqntQkBM03wYMOUvicPqeDaU6vCckAr+kpTjtCZlFkWhTLVyAtEASqcEcTYEQR8KRHuv/1JemjDplELZnHuQURkc+saiKREsxyra6YAOsmmiapiji9bLKzBiCDqEF6pqbhua9O48CH+1mCzbK2rE2iPCWI5eQwOd3WXkerh2/ekOe7ADE8Mp6CGdyX5SX93L72xptERNRPdd+I4+Znj0cZXDeI6BshhG8T0bkQwi0iotH/s9P6cDgcJ4tjLfYQQhFCeJqILhLRx5j5I8c9ATM/x8xXmPnK7n7r3l9wOBwPBW9L/gkh7NFQXP8MEd1h5gtERKP/GxO+83wI4ZkQwjNrK0tHNXE4HDPAPXV2Zj5DRFkIYY+ZG0T054no7yHrWK8AAB0ySURBVBPR14noc0T05dH/r92rryiKqLkw1A8XgeubiKgF7rPtba3LlgH01yl85zFczuqiTouLLqEQTEV9wyVYq0FqXWOSQjfHU2dFb2w0tc5bh4i1psn11rwk6ZyTqtY9KQeCjQ3JM1ep6nEsA9Hm5TNaF2+A7tyuQb64XLdrQp81YwpCS1ZlV+5TsAFVoCvbvQPcg6kqwkndLkJd3+yzBNB7M5j7yI43ehR1dg18/tpmMyiDPHMFhAvuHGiTLpKA5LnWzbd390afTza9HcfOfoGIvsLDlRUR0VdDCL/LzH9IRF9l5s8T0VtE9CvH6MvhcJwQjrMb/wMi+vkjPt8mok89jEE5HI4HjxnzxhNVRuab9RXNiY3eU6UxWwxANMH0NtZvanVF/Hqai7r/EvjJa00giTA820kCJqOmrstzMUOFQsq1uhbH63UR3StV7QWl0y8fIkKTcUDqn+i09lda3hYxPu1r2boAvaQCom/X8OnlQIRgUwYpzrgERUdzLiD+yE0fMYj4MYjnsYnSi+CYjWfcAMYYQ9Rb/dQZercBOfrOrulns14R1a5zQXj3um2Tsnkgz9wbP9E8fIMReci05NuPvrLjcDgeCHyxOxxzgpmK8ZVakx77U08TEVFvXweBrIPnECd2WOA9Bdxb6MVGRFRvCqlDbAJtAoiEUYz9H483bCaAMXNDONcCa4/CJoj1C/taPG8Dh1kPLBwcZapdBhleBwMtg9dhpx6pmdnI+zHMXWy82BSdHKouZuc8gFeezZDaG8iO8/pp8dmqL2rvsXcDcDqaiVZXmqvwHGOZHlPtilJUtCfee1nVtbpDJ9f//X/6HyeOwd/sDsecwBe7wzEn8MXucMwJZqqzR3FMtYWhmara1FzrmFUnTnS0j/WUg5p7HEPNIcKDRxGQUnjp8XE529P84fESmPYaeu8jqcjcYRRZZPZBKjBVC02994G5kdC8Vpr7kKIZ1PCpV0E3R/3d+ndhIF1u02xnYN4EkovI5rJ610EbyAKkE8fJyoPejykK8bSrVHT6tKWF4f5GHE9OL+1vdodjTuCL3eGYE8xUjCeKKYrveptZX59oQnk+EdVEVA/V86quzK9DO6PyAFEEAflBUtOmtz6Qyl/d1KHHNQhcGUCAS8vwm+0BIUjTBOvgESYWrRmOeqDdU6QZRERckbaLC6hqvNvFeD0HeZCUYL1UTKdFoe8Zw5q5vXPL1A3nJM9djHc45h6+2B2OOYEvdodjTjBjnZ1Ifl/8d2Y6RC+tLJxTNdmW5AMjY1IbgKlsL4NcbCbHGoOJrj3QHPtbB3IcgynowBAjKGoFk265n0LEWpDvLVeMzg4uuJHpfwWi71ZW3+1M5XKdedlWNSmY1IIyTmqdvQDT5FpTR1rmxZBkJJ5ilvQV53DMCXyxOxxzghMQ4x1vF7EhwMgqwlPfz66puj6IxUgCcpBqk0w+ECE8NsQTFeCFyzIRHauGG74Et0cOuhP00AN6eYqM2akP5raKMcstgqfg4pLh63vXQa6tKDW3XFkczauYVLRnY8LyHAxYe9Dt7A4jI8tyMn2Fv9kdjjmBL3aHY07gYvy7AnqHtbIsQTJF5U9UXQEBKJj1M2RadGS485WgiT56kJ6oDWJ2YoJdGpDFNa5oER/pr3NIQ1UYKbMEtSM2O/rrwDVXqWhPwUcfliFRjpNYq2VpLoFOvb4EvxSFvmeDVKwwg0zv6Hda/dF3XIx3OOYevtgdjjmBL3aHY07gOvu7EHFN0k2dfvJnVN3t26LXHbTEMyup6lu9ugDHhTbLYdrgErjKrTqYxEhMOZlIMgNdPDWklREc24i4ROm9WH5Uo95kgoL2LzQw1wlknbVI+mDWun1oS3Rc36TxLvK7aboegM4+Stv8R8z8u6PjdWb+BjO/Pvr/bvdndDh+qvF2xPhfJ6JX4PiLRPRCCOEpInphdOxwOB5RHEuMZ+aLRPSfE9HfI6L/ZvTxZ4no2VH5KzRM5fyFBzs8x9EQMbaxqoNk1s8Kt9/1W3fG5cwQQzTgZz6Yuh6I1h0QrTumXR2k0apJ8ZoAK0UHAjoywxsfBTHZNaqac+1gU7KA91sitsZ1Ld4OUuk/MiR36FB2MJD+Y8PdnsBxxdRVICDHBpqgKlMGEK1Zq0Yl8Ml1Up26KQXvxmZN7l850HOaQ46tdq+j6rr9obm0tLm8AMd9s/9DIvpbpBWCcyGEW0REo/9nj/qiw+F4NHDPxc7Mf4mINkII330nJ2Dm55j5CjNf2dzcvPcXHA7HQ8Fx3uyfIKK/zMxXiei3iOiTzPxPiOgOM18gIhr93zjqyyGE50MIz4QQnjlz5t2XfdPh+GnBcfKzf4mIvkRExMzPEtF/G0L4q8z8PxDR54joy6P/X3uI43RMQGTMUAvgtpqD/mZNXl3Q/7K+NhN1wdxWQn6xYEx0PdCPU6MsIxFFn8Udt4hMdFwJEV817ba7ee2tcfnl9Jvj8uWfeVq165Tyznr9lauq7lJD8v9d7YtOvWsiz+IY01Rrnf3UmkTf5ZkmlHjilOjYj18Env6aiTIcm8aIsoHem9htCff/zU0hksy6+lwRdHmwqwlHaEQQEh5S1NuXiejTzPw6EX16dOxwOB5RvC2nmhDCN2m4604hhG0i+tSDH5LD4XgYcA+6dzlKEA+JiAjSHC9B5NnunhYJez0RY9Ou7qPflz4yFOmNC10HiC1yS4ChUk9Bu8iY78B0Ffpa0GwfyHEPTG+tvR3Vbv38E+Py9vYdVbebiVi8XJNUz1vpvmpXYsppE923vSOmvnqlpupe/9Eb4/LTl0RlOP+zqplO5TQwfPADOXfn4GBcLky7hVg8JwMb7vlRVGMI9296czgc73L4Ync45gQuxp8grMgVYMc864HHVVsTFSwuADHE7R+pusG+iKdLsKlcNQESd9rSf5bqneleD8R42H1mE6jSRTHeBskg6UUi5WpV91EtZJBlqqpokBydeXfzjt7Nbh+IWJ8V+ju3unLuy5BSKze78Z2A1gQtxnf2Zb4vnNbZh/f2t8flb++ICvGJ5IJqt/Y+6T9tafF8efn0uBwSudftUr+LV1ZFjKdSe9Bt5EMVJUzMeOxvdodjbuCL3eGYE/hidzjmBCegs9/V2R5VAoIHC6uXd8HMtbe1p+qaYAF77boQSS4tac+yC3XRWfdu/FjV9bqg3wM3/Iq507ugw3eMV9ggE1ctjMgKJrJtAKa4Mky5nzAFK1WtU8a5KOqFTWWM5BjwWurnuo88BzNiqev2OzLGbSBzjI15rYeegmb4DJ/sG1KKGIg+bu2K2ez7L2oPt2fPgS3OkEUetMXTfG1NuOJXuK7adQZb43IZ63tWGzWN2E1vDsfcwxe7wzEncNPbA4BNuTMAz6eO8U576yfXx+XkQJtP9roi0r61e2Nc/jDr1Ee3NiRU+GB7W9VFQBSBHljVUgeqnAXvulDRIm0K5qoSvpcZDzrkKD8sPMonTei/UWrxM4BUnJkxMqgaFcV3p0X1Ajjx+8Zc1W2LeL4Zixh/KtGqUVYAwYYxU6IqFudaPF9bEI+6bkfUstfb2kPvie9fHJc/8st/Wo8xE97AdibPS4j1OPYhYGmrp5+rM6uXiIgoSiYvaX+zOxxzAl/sDsecwBe7wzEnOAGd/afD5JaCi+nOjjah9SBqrNXS/N5bm+JSeT7Tc3EHSAx6oP/tbGrdvuhIu35Xm3ESIIcoQZfFFM1ERFEuuuzZmtaBF9fE5LMJw9/v6T7SDFI2G60du2wS6OIm51wO+x1lofX5Mof9ByB9rFq9FFJJp4UxqQEBxIBE325UtVkrwLlTY76qASFIMPNIVTkOmfS/aebqO3/w++NyYqLZTp0X0tDmuuzPJMZeWoeceWXvZVWXJtuj69CkGQh/szsccwJf7A7HnMBNb28DyE9+46Z4PXW6WsyuVcWs0zNi9gD4vrOBFhdvH4hJrdcDvjQTUlam0kee6VCxBERQRs8yE9mGXGqFEf0qYGo6VZNHpG7E224qYiV63RERReBtxzD+3JgpA3igEet3Tx7jGGUcZUWbzRjc63paQqYemDML8PLbN+oEpprqmvlIwLzWXNCc9flATGAFlDs9HZl3tS8i/sqL/0HVnV6RdNSVmpRXzjym2q1fvjwun1mwCZiGF85T1GR/szsccwJf7A7HnMDF+GkwQSx7OxLosLsnHlLMRswG8Xl3T3u4leD51DPpfVog8ndA3D9ItWyaFNK/3cEeYLAK7GZbCmQkpchys4ML143BL4khr6hC2QqPBZJBwNest2GpAlJ0LyozLF6XCchBMT5NTSqrHorxMpBNI8afBaKMwszVLVDTWi29ZJbqcrx9IKJ7atQa5Oi7FvS5K0HUBNRQWvs6FcPO7avj8vn3vFfVXXz6Q8Pvx1rFQfib3eGYE/hidzjmBL7YHY45gevsU5D3te62vSuea13QqSMTJVWtgA5popPiFMgrujqCCvX0fh/MOIaooB5AHzT6ttKJQZ8vTbsCTF5ZZvIcq70KKBtdOYY647hGqN4r0gtDGqG6NH0EeBdhpmS7PxCBzp4Z77QBzDdaMHcirTevgMmrUuj52Ifjfl/vW2wS7m+oi9bjgHa3DSnm6ZrUrSxKHZv9mNaeeF/mP9Lm3rR1MPqvP0ccNz/7VSJqEVFBRHkI4RlmXieif0ZETxLRVSL6L0IIu5P6cDgcJ4u3I8b/uRDC0yGEZ0bHXySiF0IITxHRC6Njh8PxiOJ+xPjPEtGzo/JXaJgD7gv3OZ6TB0hf7V1NQLAPQS0ZpFmyvF8H+9Ku0z5QdUvgSXXQ1UEy/VTq0hSCadgEsTCImUbkLOEYy1QaMR6yrBa2D0sCP8JhnntUGXQfOpvoZPFWHbMV0KFuMq07EWSCzVLtUZgC7xzE7ZChqKd9MBXWzSswB9NnbudgQrqlw5/L8XZk0ku1QIxvCgedcShU96Xb1sFXt38yvE6bIRZx3Dd7IKJ/x8zfZebnRp+dCyHcIiIa/T97zL4cDscJ4Lhv9k+EEG4y81ki+gYz/+ie3xhh9OPwHBHRZfDtdTgcs8Wx3uwhhJuj/xtE9DtE9DEiusPMF4iIRv83Jnz3+RDCMyGEZ86cOfNgRu1wON427vlmZ+YFIopCCK1R+S8Q0X9PRF8nos8R0ZdH/7/2MAc6K6CJam/XklKITt3vSbnb0bp3D3Tx1ES9NYDYot0zEXGgb6LOTsa9sp5M1pVx/EpvNmSOqG/bVMyow6Mpr7T6KtjXSuvCCjqqMpuZPgp1bEx7Qd5FaKyyDqEBdPYiN6Y3IJvI4FqCUfw3YH4u16qqLgbX2q6N2lPbEUjmYcYI82F3RDZhz+csJOhbXtBXintDpSHuTPv56HNjRgUcR4w/R0S/w8PNk4SI/q8Qwr9h5heJ6KvM/HkieouIfuUYfTkcjhPCPRd7COFNIvroEZ9vE9GnHsagHA7Hg4d70BnutEFHPJB29rXpDb3a+n30dtOecCm0y/rag64A8bxt6vpQNwDTXpZrMX4B0h5XjfiszW3l0WV7bMR4hj4DiIWHTG9TxNYK8KUlGLxmzXpg8rKWN6SzR4E2KvQ4SkIxXpsYUR1ClcEaxnZAVD9taqvK21DfiwmWt0PAubIscRsg2J8/kCVptAlKYE4j67E4vk+e/snhmHv4Ync45gS+2B2OOcGc6uyguxlywYOtnXF535jN0NzWg2i21Ojs2K60daBDdvratRFdcNEMVxg9sQMmryg27C5I9Ih6+SE3VWh3yBgEuc0I25k+4FXBxrczAZaZBBTMgdGpA5iTKkYRxWjCGK/ZKvdwPDBzlcKcqkg83YNypd2L9LUgX3uwjD8TTG/2DEgEGUzdHuwJbB2I6W1lUS/PRg1MkeY1zaN5nLaF4G92h2NO4Ivd4ZgTzJEYD6I7eFllLR2Vtr0hXr8tI8b3gFwi7R3tTXfouK/jq7oguvcGum4A3O4DJIQ0hAxtkEcXTYgWHkVoGjOReWxtNwCURvHMwaYyBtE0inT/wN9IFeU9pttV4qM97YZjvnd5eCx9pJa8Ao6tByACazb17aRLTSGEjHLtoTbIJ3ismTEq1cMMow1RjFsduYOn29r2lsSSsqrU/BcURcN7M80U6G92h2NO4Ivd4ZgTnIAYf1cUfMi/M9azDNP0tCVQZe/Opmq3uweZVHuG5wsDYaCcGk+4ARAIJFZUh+O+rYOd5GxwtEhPRHQAJOQXqjVVh55rDKKjne14shRPBe7ig4hsd5FxJ52MmlCFHe0I1A6TMJaUvGtkUBw/bvZbUgfkZD8walMG/O1higcdwqbbOgUugFXjiYgee9P61KqH4eGDL26zjH+npa01C03wrjN3VPgA3YPO4Zh7+GJ3OOYEvtgdjjnBCejsxwwTekddi/5U9rW+PQATW3dfSCn2t3Uutr2OmNt6xqTWm2BuO6Szg/dbbaA9utDc1kutPi+6uTK9WZ0dItG6DX0LFytironht9yq6Mq7znrQgYIJ/BFUmlxvNM0cBtFsEdjU6kZpRx7MaSSNOI5g9gcGmYx/u2N0dvTYUx50Zv8Bv2OGsduTMQ/MXlAOpjfs014Jmt7svWhEMCcQgXjQ0c9OfyDnsh50d82gbnpzOBy+2B2OecEJiPEP8PclaPG27AP3myGe6ByAGA/87wf72oPuoItivFYF+uhB1z+a451Ic5cHY17rwrH19kJihAy9/AyvWgHmpFttLRSegRTCGM9hHNxUSibrWVZOSGN0yOmOJ8uMSICB6ZkSI38W0EdhSDRUlmYcn9E69nqQIrutzVWFErMnQ4WwmPnYgKCnyATJ5IrcAyosuQR8kBidZymRe8agKrW6+r73U7nv9Yoex70Nb/5mdzjmBr7YHY45gS92h2NO8C6MegNO81TztQ/ApNY90Lp4H0xqaVd0b8zfRkTUAr28Z4kkIf3vIO0fWSYiKkEvZ2M2ayE3vCFamGRus6a3HHT22y0ddfW+ZXGfrdSAptH6mPIU0xsSW4SjXWeJtLntkKlJtZsc9YbjCtbFGfRc3EcojNJ+bVfuWTvVc5pbc+ExYPcw2iXuP0wOzVNV5rQ4jNJ0sQVpoJdBf7eexe2ePAeLTb1047vjcNObw+Hwxe5wzAnedWJ8yICTfX9H1XXBS67f0WazDETwvhLjtbjf7k7mgx/0UXRHE5oW4yvIe2bSM7UmEFQQaXE9B8+v3BAkoLnnoKf7uL4n4185szwuW68zlPcsBx2DiF/y0WY4IqIyTI6qQ9NTRBNEXSLK4diKt0r0hYPOQM/HG1uQStvM1SSp9rCnWZhcB4M+VIfc9uAJ1zQmxhY8B5buAjkFcUHWjOrV7cszUVjawAfFQcfMq8z8z5n5R8z8CjP/EjOvM/M3mPn10f+14/TlcDhOBscV4/9nIvo3IYQP0jAV1CtE9EUieiGE8BQRvTA6djgcjyiOk8V1mYj+LBH9V0REIYQBEQ2Y+bNE9Oyo2VeI6JtE9IXpvQUKNBRxmWqmbgqbAgRV5G0R3Xst7SWHu+wDI4LnIFq39uR7na5ul0GAS2522bXojqma9A5wDXbLU0Od3MkwrZMR41F0hz5yQ3eNXmGFURNe3RK15PyCzPGZpsl9CvKozfxpd8XHsLxqUE7MayNWxBNHE2oQ6cyktg457zB109UdraJdh934Q154NAk2lRUc2EexxPHrKgxwOQPWD/sWTeFaOjbTKvTZgVu9EOv9+B54XGaFtWo8mECY9xLRJhH9n8z8R8z8f4xSN58LIdwaniDcIqKzx+jL4XCcEI6z2BMi+gUi+t9CCD9PRB16GyI7Mz/HzFeY+crm5ua9v+BwOB4KjrPYrxPR9RDCt0fH/5yGi/8OM18gIhr93zjqyyGE50MIz4QQnjlz5syDGLPD4XgHOE5+9tvMfI2ZPxBCeJWGOdl/OPr7HBF9efT/a/c+XUlDlZ+I2eSjnaKzhxwIH8Dc1uto77cemNsKE23W64gufnAg3yuMbr8M+jcbXfwATGxIHBlM1FsT1MaO6aM/zTMOdPgcdHFrekM9vTB1u6DXvXRb5uqXLp1S7apIShGMDjkhFXNk7hHqr9azTEXcqa6NrgzXEpk6JMXc7ct1ffetLdWuNwDSkmNyw0/T2e21qD4PRbNBGUxoy1W9tLqwHzMwWyIp9J/CnkPPmlyhzursPI1BdITj2tn/OhH9Jg9X6JtE9F/T8B5+lZk/T0RvEdGvHLMvh8NxAjjWYg8hfI+Injmi6lMPdjgOh+NhYcYedBExN0fle4sdd5F1xFTW3Qde95ZOzzQALrjcBJl0QeRvwff6Pc0f1wPT2L7xjEvBFJeD6L6QadMYTuqOMd8hYUWWTTapKTG+sGJ8PrGuBPHutU2Zt9WavtUfOrN05HiJjOjOk81OCoeyxKJpT8TPyX58dGgHqdWV6/wPb9welzHwhYgon5atdsK5rDoRx9HEOhTjrXkQTX3X2jKuxdVl1W4FAlz6NoUUiP8leC92rMkVhjWwAT6jLicYTYnIfeMdjrmBL3aHY07gi93hmBPMWGcPRHRXlzbum+pY6yNpG/T0triDpkbfzjE/mtGVe+AW2wGX2J5pt1eITt0tJhNCFmA2Y+MSO4Dht4zpDaPZrKsr6t/FhPLwGKLSDJEDmmcGUL5yTZurmmCqeXKtqeoqKmJNcMidFUkprPemyhcnvVidEvXjg1TP4wuv3hqXX7opz0BuXGKnmdsmwX4HiR5jQyoZphBnYD9oXrvW0Sbd9y0tjMtLiXaDTYH3vgfP/sCeC/YVepmuy0bjP8TtD/A3u8MxJ/DF7nDMCfhwyp2HeDLmTSL6CRGdJqKtezSfBXwcGj4OjUdhHG93DE+EEI70S5/pYh+flPlKCOEoJx0fh4/Dx/GQxuBivMMxJ/DF7nDMCU5qsT9/Que18HFo+Dg0HoVxPLAxnIjO7nA4Zg8X4x2OOcFMFzszf4aZX2XmN5h5Zmy0zPwbzLzBzC/BZzOnwmbmS8z870d03C8z86+fxFiYuc7M32Hm74/G8XdPYhwwnnjEb/i7JzUOZr7KzH/MzN9j5isnOI6HRts+s8XOzDER/a9E9J8R0YeJ6FeZ+cMzOv0/JqLPmM9Oggo7J6K/GUL4EBF9nIh+bTQHsx5LSkSfDCF8lIieJqLPMPPHT2Acd/HrNKQnv4uTGsefCyE8DaaukxjHw6NtDyHM5I+IfomI/i0cf4mIvjTD8z9JRC/B8atEdGFUvkBEr85qLDCGrxHRp09yLETUJKL/RER/5iTGQUQXRw/wJ4nod0/q3hDRVSI6bT6b6TiIaJmIfkyjvbQHPY5ZivGPE9E1OL4++uykcKJU2Mz8JBH9PBF9+yTGMhKdv0dDotBvhCGh6EnMyT8kor9FOkbmJMYRiOjfMfN3mfm5ExrHQ6Vtn+ViP4rnZC5NAcy8SES/TUR/I4RwcK/2DwMhhCKE8DQN36wfY+aPzHoMzPyXiGgjhPDdWZ/7CHwihPALNFQzf42Z/+wJjOG+aNvvhVku9utEdAmOLxLRzRme3+JYVNgPGsxcoeFC/80Qwr84ybEQEYUQ9miYzeczJzCOTxDRX2bmq0T0W0T0SWb+JycwDgoh3Bz93yCi3yGij53AOO6Ltv1emOVif5GInmLm94xYav8KEX19hue3+DoNKbCJjk2FfX/gYUD4PyKiV0II/+CkxsLMZ5h5dVRuENGfJ6IfzXocIYQvhRAuhhCepOHz8P+GEP7qrMfBzAvMvHS3TER/gYhemvU4Qgi3iegaM39g9NFd2vYHM46HvfFhNhr+IhG9RkR/QkR/Z4bn/adEdIuIMhr+en6eiE7RcGPo9dH/9RmM45dpqLr8gIi+N/r7i7MeCxH9HBH90WgcLxHRfzf6fOZzAmN6lmSDbtbz8V4i+v7o7+W7z+YJPSNPE9GV0b35l0S09qDG4R50DsecwD3oHI45gS92h2NO4Ivd4ZgT+GJ3OOYEvtgdjjmBL3aHY07gi93hmBP4Ync45gT/Px7IuskMqdNWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 124\n",
    "plt.imshow(X_train_orig[index]) #display sample training image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Layers in TF Keras \n",
    "\n",
    "In the previous assignment, you created layers manually in numpy. In TF Keras, you don't have to write code directly to create layers. Rather, TF Keras has pre-defined layers you can use. \n",
    "\n",
    "When you create a layer in TF Keras, you are creating a function that takes some input and transforms it into an output you can reuse later. Nice and easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - The Sequential API\n",
    "\n",
    "In the previous assignment, you built helper functions using `numpy` to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. Keras is a high-level abstraction built on top of TensorFlow, which allows for even more simplified and optimized model creation and training. \n",
    "\n",
    "For the first part of this assignment, you'll create a model using TF Keras' Sequential API, which allows you to build layer by layer, and is ideal for building models where each layer has **exactly one** input tensor and **one** output tensor. \n",
    "\n",
    "As you'll see, using the Sequential API is simple and straightforward, but is only appropriate for simpler, more straightforward tasks. Later in this notebook you'll spend some time building with a more flexible, powerful alternative: the Functional API. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Create the Sequential Model\n",
    "\n",
    "As mentioned earlier, the TensorFlow Keras Sequential API can be used to build simple models with layer operations that proceed in a sequential order. \n",
    "\n",
    "You can also add layers incrementally to a Sequential model with the `.add()` method, or remove them using the `.pop()` method, much like you would in a regular Python list.\n",
    "\n",
    "Actually, you can think of a Sequential model as behaving like a list of layers. Like Python lists, Sequential layers are ordered, and the order in which they are specified matters.  If your model is non-linear or contains layers with multiple inputs or outputs, a Sequential model wouldn't be the right choice!\n",
    "\n",
    "For any layer construction in Keras, you'll need to specify the input shape in advance. This is because in Keras, the shape of the weights is based on the shape of the inputs. The weights are only created when the model first sees some input data. Sequential models can be created by passing a list of layers to the Sequential constructor, like you will do in the next assignment.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - happyModel\n",
    "\n",
    "Implement the `happyModel` function below to build the following model: `ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Take help from [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [ZeroPadding2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D): padding 3, input shape 64 x 64 x 3\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 32 7x7 filters, stride 1\n",
    " - [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization): for axis 3\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Using default parameters\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 1 neuron and a sigmoid activation. \n",
    " \n",
    " \n",
    " **Hint:**\n",
    " \n",
    " Use **tfl** as shorthand for **tensorflow.keras.layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d28b191f257bdd5b70c7b8952559d5",
     "grade": false,
     "grade_id": "cell-0e56d3fc28b69aec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: happyModel\n",
    "\n",
    "def happyModel():\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code all the values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3\n",
    "            tfl.ZeroPadding2D(padding=3, data_format=\"channels_last\",input_shape=(64,64,3)),\n",
    "            ## Conv2D with 32 7x7 filters and stride of 1\n",
    "            tfl.Conv2D(filters=32,kernel_size=7, strides=1, data_format=\"channels_last\"),\n",
    "            ## BatchNormalization for axis 3\n",
    "            tfl.BatchNormalization(axis=3),\n",
    "            ## ReLU\n",
    "            tfl.ReLU(),\n",
    "            ## Max Pooling 2D with default parameters\n",
    "            tfl.MaxPool2D(),\n",
    "            ## Flatten layer\n",
    "            tfl.Flatten(data_format=\"channels_last\"),\n",
    "            ## Dense layer with 1 unit for output & 'sigmoid' activation\n",
    "            tfl.Dense(units=1,activation='sigmoid')\n",
    "            # YOUR CODE STARTS HERE\n",
    "            \n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "            ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d3575c950e2e78149be2d05d671c80d",
     "grade": true,
     "grade_id": "cell-e3e1046e5c33d775",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
      "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
      "['BatchNormalization', (None, 64, 64, 32), 128]\n",
      "['ReLU', (None, 64, 64, 32), 0]\n",
      "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
      "['Flatten', (None, 32768), 0]\n",
      "['Dense', (None, 1), 32769, 'sigmoid']\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "happy_model = happyModel()\n",
    "# Print a summary for each layer\n",
    "for layer in summary(happy_model):\n",
    "    print(layer)\n",
    "    \n",
    "output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],\n",
    "            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],\n",
    "            ['BatchNormalization', (None, 64, 64, 32), 128],\n",
    "            ['ReLU', (None, 64, 64, 32), 0],\n",
    "            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],\n",
    "            ['Flatten', (None, 32768), 0],\n",
    "            ['Dense', (None, 1), 32769, 'sigmoid']]\n",
    "    \n",
    "comparator(summary(happy_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
    "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
    "['BatchNormalization', (None, 64, 64, 32), 128]\n",
    "['ReLU', (None, 64, 64, 32), 0]\n",
    "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
    "['Flatten', (None, 32768), 0]\n",
    "['Dense', (None, 1), 32769, 'sigmoid']\n",
    "All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is created, you can compile it for training with an optimizer and loss of your choice. When the string `accuracy` is specified as a metric, the type of accuracy used will be automatically converted based on the loss function used. This is one of the many optimizations built into TensorFlow that make your life easier! If you'd like to read more on how the compiler operates, check the docs [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to check your model's parameters with the `.summary()` method. This will display the types of layers you have, the shape of the outputs, and how many parameters are in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_12 (ZeroPaddi (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "happy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Train and Evaluate the Model\n",
    "\n",
    "After creating the model, compiling it with your choice of optimizer and loss function, and doing a sanity check on its contents, you are now ready to build! \n",
    "\n",
    "Simply call `.fit()` to train. That's it! No need for mini-batching, saving, or complex backpropagation computations. That's all been done for you, as you're using a TensorFlow dataset with the batches specified already. You do have the option to specify epoch number or minibatch size if you like (for example, in the case of an un-batched dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.9675 - accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.2418 - accuracy: 0.9017\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1934 - accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.2307 - accuracy: 0.9183\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0743 - accuracy: 0.9767\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0983 - accuracy: 0.9683\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.1380 - accuracy: 0.9500\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0777 - accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0818 - accuracy: 0.9650\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0700 - accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd560298c50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that completes, just use `.evaluate()` to evaluate against your test set. This function will print the value of the loss function and the performance metrics specified during the compilation of the model. In this case, the `binary_crossentropy` and the `accuracy` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4898 - accuracy: 0.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48975899815559387, 0.7866666913032532]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? But what if you need to build a model with shared layers, branches, or multiple inputs and outputs? This is where Sequential, with its beautifully simple yet limited functionality, won't be able to help you. \n",
    "\n",
    "Next up: Enter the Functional API, your slightly more complex, highly flexible friend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - The Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second half of the assignment, where you'll use Keras' flexible [Functional API](https://www.tensorflow.org/guide/keras/functional) to build a ConvNet that can differentiate between 6 sign language digits. \n",
    "\n",
    "The Functional API can handle models with non-linear topology, shared layers, as well as layers with multiple inputs or outputs. Imagine that, where the Sequential API requires the model to move in a linear fashion through its layers, the Functional API allows much more flexibility. Where Sequential is a straight line, a Functional model is a graph, where the nodes of the layers can connect in many more ways than one. \n",
    "\n",
    "In the visual example below, the one possible direction of the movement Sequential model is shown in contrast to a skip connection, which is just one of the many ways a Functional model can be constructed. A skip connection, as you might have guessed, skips some layer in the network and feeds the output to a later layer in the network. Don't worry, you'll be spending more time with skip connections very soon! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq_vs_func.png\" style=\"width:350px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Load the SIGNS Dataset\n",
    "\n",
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of an image from the dataset\n",
    "index = 9\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Split the Data into Train/Test Sets\n",
    "\n",
    "In Course 2, you built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward Propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that implement the convolution steps for you. By now, you should be familiar with how TensorFlow builds computational graphs. In the [Functional API](https://www.tensorflow.org/guide/keras/functional), you create a graph of layers. This is what allows such great flexibility.\n",
    "\n",
    "However, the following model could also be defined using the Sequential API since the information flow is on a single line. But don't deviate. What we want you to learn is to use the functional API.\n",
    "\n",
    "Begin building your graph of layers by creating an input node that functions as a callable object:\n",
    "\n",
    "- **input_img = tf.keras.Input(shape=input_shape):** \n",
    "\n",
    "Then, create a new node in the graph of layers by calling a layer on the `input_img` object: \n",
    "\n",
    "- **tf.keras.layers.Conv2D(filters= ... , kernel_size= ... , padding='same')(input_img):** Read the full documentation on [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "- **tf.keras.layers.MaxPool2D(pool_size=(f, f), strides=(s, s), padding='same'):** `MaxPool2D()` downsamples your input using a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, you usually operate on a single example at a time and a single channel at a time. Read the full documentation on [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D).\n",
    "\n",
    "- **tf.keras.layers.ReLU():** computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU).\n",
    "\n",
    "- **tf.keras.layers.Flatten()**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "\n",
    "    * If a tensor P has the shape (batch_size,h,w,c), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    \n",
    "    * For example, given a tensor with dimensions [100, 2, 3, 4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    "\n",
    "- **tf.keras.layers.Dense(units= ... , activation='softmax')(F):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n",
    "In the last function above (`tf.keras.layers.Dense()`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "Lastly, before creating the model, you'll need to define the output using the last of the function's compositions (in this example, a Dense layer): \n",
    "\n",
    "- **outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)**\n",
    "\n",
    "\n",
    "#### Window, kernel, filter, pool\n",
    "\n",
    "The words \"kernel\" and \"filter\" are used to refer to the same thing. The word \"filter\" accounts for the amount of \"kernels\" that will be used in a single convolution layer. \"Pool\" is the name of the operation that takes the max or average value of the kernels. \n",
    "\n",
    "This is why the parameter `pool_size` refers to `kernel_size`, and you use `(f,f)` to refer to the filter size. \n",
    "\n",
    "Pool size and kernel size refer to the same thing in different objects - They refer to the shape of the window where the operation takes place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - convolutional_model\n",
    "\n",
    "Implement the `convolutional_model` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Use the functions above! \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 8 4 by 4 filters, stride 1, padding is \"SAME\"\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    " - **Conv2D**: Use 16 2 by 2 filters, stride 1, padding is \"SAME\"\n",
    " - **ReLU**\n",
    " - **MaxPool2D**: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 6 neurons and a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58643806aa8380c96225fc8b4c5e7aa",
     "grade": false,
     "grade_id": "cell-dac51744a9e03f51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convolutional_model\n",
    "\n",
    "def convolutional_model(input_shape):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code some values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    input_img -- input dataset, of shape (input_shape)\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    ## CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'\n",
    "    # Z1 = None\n",
    "    Z1=tfl.Conv2D(filters=8,kernel_size=4,strides=1,padding='same')(input_img)\n",
    "    ## RELU\n",
    "    A1 = tfl.ReLU()(Z1)\n",
    "    ## MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    # P1 = None\n",
    "    P1=tfl.MaxPool2D(pool_size=8,strides=8,padding='same')(A1)\n",
    "    ## CONV2D: 16 filters 2x2, stride 1, padding 'SAME'\n",
    "    # Z2 = None\n",
    "    Z2=tfl.Conv2D(filters=16,kernel_size=2,strides=1,padding='same')(P1)\n",
    "    ## RELU\n",
    "    # A2 = None\n",
    "    A2=tfl.ReLU()(Z2)\n",
    "    ## MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    # P2 = None\n",
    "    P2=tfl.MaxPool2D(pool_size=4,strides=4,padding='same')(A2)\n",
    "    ## FLATTEN\n",
    "    # F = None\n",
    "    F=tfl.Flatten()(P2)\n",
    "    ## Dense layer\n",
    "    ## 6 neurons in output layer. Hint: one of the arguments should be \"activation='softmax'\" \n",
    "    # outputs = None\n",
    "    FC1=tfl.Dense(units=6,activation='softmax')(F)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    outputs=tfl.Dropout(rate=0.3)(FC1)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "483d626949930a0b0ef20997e7c6ba72",
     "grade": true,
     "grade_id": "cell-45d22e92042174c9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 64, 8)         392       \n",
      "_________________________________________________________________\n",
      "re_lu_18 (ReLU)              (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 8, 16)          528       \n",
      "_________________________________________________________________\n",
      "re_lu_19 (ReLU)              (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,310\n",
      "Trainable params: 1,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\u001b[31mTest failed. Your output is not as expected output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conv_model = convolutional_model((64, 64, 3))\n",
    "conv_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "conv_model.summary()\n",
    "    \n",
    "output = [['InputLayer', [(None, 64, 64, 3)], 0],\n",
    "        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 64, 64, 8), 0],\n",
    "        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],\n",
    "        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 8, 8, 16), 0],\n",
    "        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],\n",
    "        ['Flatten', (None, 64), 0],\n",
    "        ['Dense', (None, 6), 390, 'softmax']]\n",
    "    \n",
    "comparator(summary(conv_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Sequential and Functional APIs return a TF Keras model object. The only difference is how inputs are handled inside the object model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.9035 - accuracy: 0.1454 - val_loss: 1.7950 - val_accuracy: 0.1333\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1648 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - 2s 101ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 27/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 55/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - 2s 105ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 83/300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 88/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 89/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 90/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 91/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 92/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 93/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 94/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 95/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 96/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 97/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 98/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 99/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 100/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 101/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 102/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 103/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 104/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 105/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 106/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 107/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 108/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 109/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 110/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 111/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 112/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 113/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 114/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 115/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 116/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 117/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 118/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 120/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 121/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 122/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 123/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 124/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 125/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 126/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 127/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 128/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 129/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 130/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 131/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 132/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 133/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 134/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 135/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 136/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 137/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 138/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 139/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 140/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 141/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 142/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 143/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 144/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 145/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 146/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 147/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 148/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 149/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 150/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 151/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 152/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 153/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 154/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 155/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 156/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 157/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 158/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 159/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 160/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 161/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 162/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 163/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 164/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 165/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 166/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 167/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 168/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 169/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 170/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 171/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 172/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 173/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 174/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 175/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 176/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 177/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 179/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 180/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 181/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 182/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 183/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 184/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 185/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 186/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 187/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 188/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 189/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 190/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 191/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 192/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 193/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 194/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 195/300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 196/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 197/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 198/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 199/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 200/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 201/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 202/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 203/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 204/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 205/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 206/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 207/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 208/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 209/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 210/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 211/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 212/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 213/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 214/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 215/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 216/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 217/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 218/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 219/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 220/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 221/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 222/300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 223/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 224/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 225/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 226/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 227/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 228/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 229/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 230/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 231/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 232/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 233/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 234/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 235/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 236/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 238/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 239/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 240/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 241/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 242/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 243/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 244/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 245/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 246/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 247/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 248/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 249/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 250/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 251/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 252/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 253/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 254/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 255/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 256/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 257/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 258/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 259/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 260/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 261/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 262/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 263/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 264/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 265/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 266/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 267/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 268/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 269/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 270/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 271/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 272/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 273/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 274/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 275/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 276/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 277/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 278/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 279/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 280/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 281/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 282/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 283/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 284/300\n",
      "17/17 [==============================] - 2s 101ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 285/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 286/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 287/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 288/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 289/300\n",
      "17/17 [==============================] - 2s 107ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 290/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 291/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 292/300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 293/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 294/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 295/300\n",
      "17/17 [==============================] - 2s 101ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 297/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 298/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 299/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 300/300\n",
      "17/17 [==============================] - 2s 106ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)\n",
    "history = conv_model.fit(train_dataset, epochs=300, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - History Object \n",
    "\n",
    "The history object is an output of the `.fit()` operation, and provides a record of all the loss and metric values in memory. It's stored as a dictionary that you can retrieve at `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.8120349645614624,\n",
       "  1.791825771331787,\n",
       "  1.7890093326568604,\n",
       "  1.7851067781448364,\n",
       "  1.780941367149353,\n",
       "  1.7764003276824951,\n",
       "  1.7713888883590698,\n",
       "  1.7652513980865479,\n",
       "  1.7575831413269043,\n",
       "  1.7476778030395508,\n",
       "  1.7349019050598145,\n",
       "  1.719475269317627,\n",
       "  1.7004003524780273,\n",
       "  1.6795926094055176,\n",
       "  1.6494543552398682,\n",
       "  1.6154390573501587,\n",
       "  1.577052354812622,\n",
       "  1.530930519104004,\n",
       "  1.4785757064819336,\n",
       "  1.4220091104507446,\n",
       "  1.3712483644485474,\n",
       "  1.3235430717468262,\n",
       "  1.2813994884490967,\n",
       "  1.2393194437026978,\n",
       "  1.2007180452346802,\n",
       "  1.1706488132476807,\n",
       "  1.1439777612686157,\n",
       "  1.1097968816757202,\n",
       "  1.0912927389144897,\n",
       "  1.0692241191864014,\n",
       "  1.0413317680358887,\n",
       "  1.0258299112319946,\n",
       "  1.0058938264846802,\n",
       "  0.987297773361206,\n",
       "  0.9708631634712219,\n",
       "  0.955851137638092,\n",
       "  0.9416008591651917,\n",
       "  0.9274769425392151,\n",
       "  0.9147093892097473,\n",
       "  0.9022791981697083,\n",
       "  0.8900758028030396,\n",
       "  0.879046618938446,\n",
       "  0.8678631782531738,\n",
       "  0.8572783470153809,\n",
       "  0.8468015193939209,\n",
       "  0.8368899822235107,\n",
       "  0.8269305229187012,\n",
       "  0.817681074142456,\n",
       "  0.8083475232124329,\n",
       "  0.7996094822883606,\n",
       "  0.790863573551178,\n",
       "  0.7824791669845581,\n",
       "  0.7743048667907715,\n",
       "  0.7664602994918823,\n",
       "  0.7581830620765686,\n",
       "  0.7507507801055908,\n",
       "  0.743253767490387,\n",
       "  0.735386312007904,\n",
       "  0.7286717891693115,\n",
       "  0.7211008071899414,\n",
       "  0.7146174907684326,\n",
       "  0.7073941230773926,\n",
       "  0.7012045383453369,\n",
       "  0.6945068836212158,\n",
       "  0.6883776187896729,\n",
       "  0.6819970607757568,\n",
       "  0.675682008266449,\n",
       "  0.669708788394928,\n",
       "  0.663938581943512,\n",
       "  0.6577513813972473,\n",
       "  0.652126669883728,\n",
       "  0.6460821032524109,\n",
       "  0.6407965421676636,\n",
       "  0.6350383162498474,\n",
       "  0.6297834515571594,\n",
       "  0.6241510510444641,\n",
       "  0.6191388368606567,\n",
       "  0.6137939095497131,\n",
       "  0.6087166666984558,\n",
       "  0.6034099459648132,\n",
       "  0.5985835790634155,\n",
       "  0.5934252738952637,\n",
       "  0.588732898235321,\n",
       "  0.5837723016738892,\n",
       "  0.5795823931694031,\n",
       "  0.5745841264724731,\n",
       "  0.5702060461044312,\n",
       "  0.5657350420951843,\n",
       "  0.5612940192222595,\n",
       "  0.5567370653152466,\n",
       "  0.5524303913116455,\n",
       "  0.5482218861579895,\n",
       "  0.5439426898956299,\n",
       "  0.539887547492981,\n",
       "  0.5359804630279541,\n",
       "  0.5318884253501892,\n",
       "  0.5278549790382385,\n",
       "  0.5241320133209229,\n",
       "  0.5199993252754211,\n",
       "  0.5165941119194031],\n",
       " 'accuracy': [0.1537037044763565,\n",
       "  0.1509259194135666,\n",
       "  0.17777778208255768,\n",
       "  0.2231481522321701,\n",
       "  0.2611111104488373,\n",
       "  0.25740739703178406,\n",
       "  0.28333333134651184,\n",
       "  0.3046296238899231,\n",
       "  0.3342592716217041,\n",
       "  0.364814817905426,\n",
       "  0.4018518626689911,\n",
       "  0.42222222685813904,\n",
       "  0.4185185134410858,\n",
       "  0.40925925970077515,\n",
       "  0.43611112236976624,\n",
       "  0.4574074149131775,\n",
       "  0.4749999940395355,\n",
       "  0.47962963581085205,\n",
       "  0.5037037134170532,\n",
       "  0.5324074029922485,\n",
       "  0.5444444417953491,\n",
       "  0.5666666626930237,\n",
       "  0.5657407641410828,\n",
       "  0.5870370268821716,\n",
       "  0.6018518805503845,\n",
       "  0.6037036776542664,\n",
       "  0.6111111044883728,\n",
       "  0.6240741014480591,\n",
       "  0.625,\n",
       "  0.6333333253860474,\n",
       "  0.6472222208976746,\n",
       "  0.654629647731781,\n",
       "  0.664814829826355,\n",
       "  0.6657407283782959,\n",
       "  0.6759259104728699,\n",
       "  0.6842592358589172,\n",
       "  0.6879629492759705,\n",
       "  0.6990740895271301,\n",
       "  0.7037037014961243,\n",
       "  0.7074074149131775,\n",
       "  0.7138888835906982,\n",
       "  0.7250000238418579,\n",
       "  0.7222222089767456,\n",
       "  0.7277777791023254,\n",
       "  0.7287036776542664,\n",
       "  0.7324073910713196,\n",
       "  0.730555534362793,\n",
       "  0.7333333492279053,\n",
       "  0.7370370626449585,\n",
       "  0.7435185313224792,\n",
       "  0.7453703880310059,\n",
       "  0.7490741014480591,\n",
       "  0.7481481432914734,\n",
       "  0.7518518567085266,\n",
       "  0.7555555701255798,\n",
       "  0.7583333253860474,\n",
       "  0.7611111402511597,\n",
       "  0.7629629373550415,\n",
       "  0.7657407522201538,\n",
       "  0.7712963223457336,\n",
       "  0.7712963223457336,\n",
       "  0.7740740776062012,\n",
       "  0.7740740776062012,\n",
       "  0.7749999761581421,\n",
       "  0.7768518328666687,\n",
       "  0.7777777910232544,\n",
       "  0.7777777910232544,\n",
       "  0.7787036895751953,\n",
       "  0.779629647731781,\n",
       "  0.7861111164093018,\n",
       "  0.7870370149612427,\n",
       "  0.7879629731178284,\n",
       "  0.7907407283782959,\n",
       "  0.7925925850868225,\n",
       "  0.7944444417953491,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.8027777671813965,\n",
       "  0.8027777671813965,\n",
       "  0.8074073791503906,\n",
       "  0.8083333373069763,\n",
       "  0.8101851940155029,\n",
       "  0.8101851940155029,\n",
       "  0.8111110925674438,\n",
       "  0.8138889074325562,\n",
       "  0.8185185194015503,\n",
       "  0.8194444179534912,\n",
       "  0.8212962746620178,\n",
       "  0.8222222328186035,\n",
       "  0.8231481313705444,\n",
       "  0.8231481313705444,\n",
       "  0.824999988079071,\n",
       "  0.8259259462356567,\n",
       "  0.8277778029441833,\n",
       "  0.8314814567565918,\n",
       "  0.8296296000480652,\n",
       "  0.8324074149131775,\n",
       "  0.8333333134651184,\n",
       "  0.8370370268821716,\n",
       "  0.8425925970077515],\n",
       " 'val_loss': [1.7915395498275757,\n",
       "  1.787627100944519,\n",
       "  1.78512704372406,\n",
       "  1.7818635702133179,\n",
       "  1.7791725397109985,\n",
       "  1.7762761116027832,\n",
       "  1.772145390510559,\n",
       "  1.7672821283340454,\n",
       "  1.760073184967041,\n",
       "  1.7498306035995483,\n",
       "  1.7385385036468506,\n",
       "  1.7259161472320557,\n",
       "  1.7102153301239014,\n",
       "  1.6867579221725464,\n",
       "  1.6633868217468262,\n",
       "  1.6362916231155396,\n",
       "  1.6028882265090942,\n",
       "  1.568660855293274,\n",
       "  1.5170501470565796,\n",
       "  1.4660600423812866,\n",
       "  1.4264907836914062,\n",
       "  1.3802096843719482,\n",
       "  1.3441271781921387,\n",
       "  1.3042641878128052,\n",
       "  1.2663251161575317,\n",
       "  1.234059453010559,\n",
       "  1.2070797681808472,\n",
       "  1.1806367635726929,\n",
       "  1.1558761596679688,\n",
       "  1.1311160326004028,\n",
       "  1.110217809677124,\n",
       "  1.0932224988937378,\n",
       "  1.0757230520248413,\n",
       "  1.055291771888733,\n",
       "  1.0417991876602173,\n",
       "  1.0254225730895996,\n",
       "  1.0117627382278442,\n",
       "  0.9973803162574768,\n",
       "  0.9857993721961975,\n",
       "  0.9722471237182617,\n",
       "  0.9616495370864868,\n",
       "  0.9492612481117249,\n",
       "  0.9395186901092529,\n",
       "  0.9287022948265076,\n",
       "  0.9190757870674133,\n",
       "  0.9086799621582031,\n",
       "  0.8995805978775024,\n",
       "  0.8904204964637756,\n",
       "  0.8821841478347778,\n",
       "  0.8735160827636719,\n",
       "  0.8644173741340637,\n",
       "  0.8563091158866882,\n",
       "  0.8485453128814697,\n",
       "  0.8406684994697571,\n",
       "  0.8330758213996887,\n",
       "  0.8247696161270142,\n",
       "  0.8177427649497986,\n",
       "  0.809901773929596,\n",
       "  0.8030167818069458,\n",
       "  0.7960265278816223,\n",
       "  0.7889164090156555,\n",
       "  0.7824534177780151,\n",
       "  0.7753780484199524,\n",
       "  0.7690078616142273,\n",
       "  0.7625340819358826,\n",
       "  0.7563169002532959,\n",
       "  0.7506239414215088,\n",
       "  0.744235098361969,\n",
       "  0.7383033633232117,\n",
       "  0.7312299013137817,\n",
       "  0.7256613969802856,\n",
       "  0.719161331653595,\n",
       "  0.7139348983764648,\n",
       "  0.7076895236968994,\n",
       "  0.7028148770332336,\n",
       "  0.6973528265953064,\n",
       "  0.6926320195198059,\n",
       "  0.6871029734611511,\n",
       "  0.68266361951828,\n",
       "  0.6770698428153992,\n",
       "  0.6734341382980347,\n",
       "  0.6686854958534241,\n",
       "  0.6643520593643188,\n",
       "  0.6592311859130859,\n",
       "  0.6556687951087952,\n",
       "  0.6509568691253662,\n",
       "  0.6469541192054749,\n",
       "  0.6427955627441406,\n",
       "  0.638690173625946,\n",
       "  0.6344950199127197,\n",
       "  0.6304339170455933,\n",
       "  0.6271533966064453,\n",
       "  0.6229291558265686,\n",
       "  0.6191508173942566,\n",
       "  0.6156653761863708,\n",
       "  0.6123889684677124,\n",
       "  0.608646035194397,\n",
       "  0.6054295897483826,\n",
       "  0.6015979051589966,\n",
       "  0.5992844104766846],\n",
       " 'val_accuracy': [0.1666666716337204,\n",
       "  0.14166666567325592,\n",
       "  0.20000000298023224,\n",
       "  0.2750000059604645,\n",
       "  0.2916666567325592,\n",
       "  0.32499998807907104,\n",
       "  0.30000001192092896,\n",
       "  0.3499999940395355,\n",
       "  0.34166666865348816,\n",
       "  0.4166666567325592,\n",
       "  0.44999998807907104,\n",
       "  0.4583333432674408,\n",
       "  0.38333332538604736,\n",
       "  0.4166666567325592,\n",
       "  0.4166666567325592,\n",
       "  0.40833333134651184,\n",
       "  0.4333333373069763,\n",
       "  0.4583333432674408,\n",
       "  0.4583333432674408,\n",
       "  0.5166666507720947,\n",
       "  0.5333333611488342,\n",
       "  0.550000011920929,\n",
       "  0.5166666507720947,\n",
       "  0.5166666507720947,\n",
       "  0.5249999761581421,\n",
       "  0.5333333611488342,\n",
       "  0.5166666507720947,\n",
       "  0.5166666507720947,\n",
       "  0.5249999761581421,\n",
       "  0.5416666865348816,\n",
       "  0.5416666865348816,\n",
       "  0.550000011920929,\n",
       "  0.550000011920929,\n",
       "  0.5666666626930237,\n",
       "  0.574999988079071,\n",
       "  0.5916666388511658,\n",
       "  0.6000000238418579,\n",
       "  0.5916666388511658,\n",
       "  0.5916666388511658,\n",
       "  0.6083333492279053,\n",
       "  0.6166666746139526,\n",
       "  0.6333333253860474,\n",
       "  0.6333333253860474,\n",
       "  0.6333333253860474,\n",
       "  0.6333333253860474,\n",
       "  0.6416666507720947,\n",
       "  0.6583333611488342,\n",
       "  0.6499999761581421,\n",
       "  0.6583333611488342,\n",
       "  0.6499999761581421,\n",
       "  0.6583333611488342,\n",
       "  0.6499999761581421,\n",
       "  0.6583333611488342,\n",
       "  0.6583333611488342,\n",
       "  0.675000011920929,\n",
       "  0.675000011920929,\n",
       "  0.675000011920929,\n",
       "  0.6833333373069763,\n",
       "  0.6916666626930237,\n",
       "  0.6833333373069763,\n",
       "  0.6833333373069763,\n",
       "  0.675000011920929,\n",
       "  0.675000011920929,\n",
       "  0.6833333373069763,\n",
       "  0.675000011920929,\n",
       "  0.6833333373069763,\n",
       "  0.6916666626930237,\n",
       "  0.6916666626930237,\n",
       "  0.6916666626930237,\n",
       "  0.699999988079071,\n",
       "  0.7166666388511658,\n",
       "  0.7166666388511658,\n",
       "  0.7250000238418579,\n",
       "  0.7250000238418579,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7416666746139526,\n",
       "  0.7416666746139526,\n",
       "  0.7416666746139526,\n",
       "  0.75,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7749999761581421,\n",
       "  0.7833333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.7916666865348816,\n",
       "  0.7916666865348816,\n",
       "  0.7833333611488342]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the loss over time using `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Accuracy'), Text(0.5, 0, 'Epoch')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
    "# model was trained on. \n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You've finished the assignment and built two models: One that recognizes  smiles, and another that recognizes SIGN language with almost 80% accuracy on the test set. In addition to that, you now also understand the applications of two Keras APIs: Sequential and Functional. Nicely done! \n",
    "\n",
    "By now, you know a bit about how the Functional API works and may have glimpsed the possibilities. In your next assignment, you'll really get a feel for its power when you get the opportunity to build a very deep ConvNet, using ResNets! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Bibliography\n",
    "\n",
    "You're always encouraged to read the official documentation. To that end, you can find the docs for the Sequential and Functional APIs here: \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/sequential_model\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/functional"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
